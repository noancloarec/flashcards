[
    {
        "id": "deck_1",
        "name": "Building resilient streaming systems with Google Cloud",
        "cards": [
            {
                "question": "Would you use different pub-sub instances for different projects ?",
                "answer": "No, only one pub-sub for an organization, then you will create topics to separate data"
            },
            {
                "question": "In Pub Sub, what is the difference between the push and pull delivery ?",
                "answer": "In pull delivery, the subscriber will ask pub sub what are the new messages whereas in push delivery the subscriber will have a webhook called by pub sub so that delivery is almost instantaneous."
            },
            {
                "question": "In Pub Sub, what happens if a subscriber never sends acknowledgements after receiving messages ?",
                "answer": "The message is considered outstanding and will be redelivered after a certain amount of time, which grows exponentially with the number of retries."
            },
            {
                "question": "In Pub Sub, can one subscription have many subscribers ?",
                "answer": "Yes"
            },
            {
                "question": "In Pub Sub, by default how long are messages conserved ?",
                "answer": "For 7 days"
            },
            {
                "question": "In DataFlow, what is a watermark ?",
                "answer": "It is a threshold that indicates when DataFlow expects all of the data in a window to have arrived."
            },
            {
                "question": "What timestamp does DataFlow take for an event to decide which window to put it in ?",
                "answer": "The event creation time, may be generated by pub-sub or user-provided."
            },
            {
                "question": "How to handle late event in DataFlow when we do not want to set a too big watermark ?",
                "answer": "By setting up a trigger which will fire early, at the watermark, and when late data arrives."
            },
            {
                "question": "What does ACID mean ?",
                "answer": "Atomicity, Consistency, Isolation, Durability"
            },
            {
                "question": "What is Colossus ?",
                "answer": "It is the Google File System that powers several Google Cloud products such as Cloud Storage or Big Table"
            },
            {
                "question": "What is the most important thing to do when designing BigTable schema regarding performance ?",
                "answer": "Designing the key so that reads and writes are shared evenly accross the key range, and that one read does not require many tablets to be read"
            },
            {
                "question": "In BigTable, what can you do to optimize performance if most requests concern recent data ?",
                "answer": "Use reverse timestamp as they key so that the most recent records are at the top of the file."
            },
            {
                "question": "In BigTable, at which level is an operation atomic ?",
                "answer": "At the row level, i.e. if an operation over 2 rows modifies one row and fails at the 2nd the change on the first row won't be reversed as in common ACID db systems"
            },
            {
                "question": "In BigTable, what is a column family ?",
                "answer": "A set of column in a table that are often queried together, grouping these columns well can help to limit what you're pulling back"
            },
            {
                "question": "What does BigTable do to automatically optimize performance ?",
                "answer": "It scans your access patterns and distribute data (or pointers to data) accross nodes in the cluster to distribute workloads evenly."
            }
        ]
    },
    {
        "id": "deck_2",
        "name": "Serverless data processing with DataFlow : Foundations",
        "cards": [
            {
                "question": "What is the Beam portability framework ?",
                "answer": "A set of tools to write parts of pipelines in your preferred language and use it in another languages"
            },
            {
                "question": "In Dataflow, what is Flexible Resource Scheduling (FlexRS) ?",
                "answer": "An option to reduce cost of pipelines as they will wait (up to 6 hours) for resources to be available before starting the job."
            }
        ]
    },
    {
        "id": "deck_3",
        "name": "Build batch data pipelines on Google Cloud",
        "cards": [
            {
                "question": "When to use EL ?",
                "answer": "When data do not need transformation"
            },
            {
                "question": "When to use ELT ?",
                "answer": "When data transformations can be expressed in SQL"
            },
            {
                "question": "When to use ETL ?",
                "answer": "When data transformation cannot be expressed in SQL"
            },
            {
                "question": "For most ETL jobs, using DataFlow and query should be enough. What are the reason where you would need to look beyond these tools ?",
                "answer": "Need of low latency and high throughput, need to reuse Spark Pipelines, need for visual data pipeline building."
            },
            {
                "question": "Which tool to use to build ETLs when needing low latency and high throughput (more than 10M rows / sec) ?",
                "answer": "DataFlow to Big Table"
            },
            {
                "question": "Which tool to use to build ETLs when needing to reuse Spark Pipelines ?",
                "answer": "Dataproc"
            },
            {
                "question": "Which tool to use when needing to design pipelines visually ?",
                "answer": "Cloud Data Fusion"
            },
            {
                "question": "Which tool to use to provide data lineage ?",
                "answer": "Data Catalog"
            },
            {
                "question": "Which tool to use to protect sensitive data ?",
                "answer": "Cloud Data Loss Prevention"
            },
            {
                "question": "What is a major limitation when executing jobs on a hadoop cluster ?",
                "answer": "There is no separation between compute and storage (it is hard to scale fast)"
            },
            {
                "question": "When using Dataproc, why shouldn't you use hadoop commands directly but instead prefer Google Cloud commands ?",
                "answer": "To allow Dataproc to collect metadata for cluster management purposes"
            },
            {
                "question": "Jobs are not restartable by default, if you enable job restart what should you do when designing them ?",
                "answer": "You should make sure they are idempotent"
            },
            {
                "question": "What are some steps you can take to optimize a dataproc job ?",
                "answer": "Make sure it is close to cloud storage, that there are no funnels, les than 10K input files, benchmark the disk size and computing power. You can also create a cluster for a job"
            },
            {
                "question": "What are the few reasons to use HDFS with dataproc ?",
                "answer": "Frequent directory renaming, data appending to files, heavy IO, low latency needs."
            },
            {
                "question": "When working with dataproc, what is an IDLE cluster ?",
                "answer": "A cluster not working"
            },
            {
                "question": "Can dataproc auto-scaling scale to zero ?",
                "answer": "No, you may want to terminate a temporary cluster instead of scaling it to zero"
            },
            {
                "question": "Which open-source project is DataFlow based on ?",
                "answer": "Apache Beam"
            },
            {
                "question": "In DataFlow, what is a P Collection ?",
                "answer": "An immutable dataset"
            },
            {
                "question": "In DataFlow, what is a P Transform ?",
                "answer": "An action (code) performed on a P Collection"
            },
            {
                "question": "In DataFlow, what is a bounded collection ?",
                "answer": "A batch dataset"
            },
            {
                "question": "In DataFlow, what is an unbounded collection ?",
                "answer": "A streaming dataset"
            },
            {
                "question": "In DataFlow, why is data stored as a serialized byte string ?",
                "answer": "To avoid serialization/deserialization operations"
            },
            {
                "question": "When programming a DataFlow pipeline with Python, how do you chain P transforms ?",
                "answer": "By using the pipe '|' operator"
            },
            {
                "question": "In DataFlow, what is a side input ?",
                "answer": "An additional inputs that fits in memory (can be an aggregation provided by a preceeding P Transform)"
            },
            {
                "question": "In DataFlow, why should you use Combine instead of GroupBy when performing a reduction ?",
                "answer": "GroupBy will shuffle elements and one worker may have many more elements to compute than another. Combine requires an associative and commutative function but avoid the shuffling."
            },
            {
                "question": "Which software is Cloud Composer based on ?",
                "answer": "Apache Airflow"
            },
            {
                "question": "Which tool to use to build pipelines in no-code ?",
                "answer": "Cloud Data Fusion"
            },
            {
                "question": "What are the different ways to schedule jobs in Airflow ?",
                "answer": "On a periodic basis or with triggers"
            },
            {
                "question": "What are 5 criteria to consider when it comes to data quality ?",
                "answer": "Validity, Accuracy, Completeness, Consistency, Uniformity"
            }
        ]
    },
    {
        "id": "deck_4",
        "name": "Modernizing datalake and data warehouse with Google Cloud",
        "cards": [
            {
                "question": "When to use EL ?",
                "answer": "When data do not need transformation"
            },
            {
                "question": "When to use ELT ?",
                "answer": "When data transformations can be expressed in SQL"
            },
            {
                "question": "When to use ETL ?",
                "answer": "When data transformation cannot be expressed in SQL"
            },
            {
                "question": "For most ETL jobs, using DataFlow and query should be enough. What are the reason where you would need to look beyond these tools ?",
                "answer": "Need of low latency and high throughput, need to reuse Spark Pipelines, need for visual data pipeline building."
            },
            {
                "question": "Which tool to use to build ETLs when needing low latency and high throughput (more than 10M rows / sec) ?",
                "answer": "DataFlow to Big Table"
            },
            {
                "question": "Which tool to use to build ETLs when needing to reuse Spark Pipelines ?",
                "answer": "Dataproc"
            },
            {
                "question": "Which tool to use when needing to design pipelines visually ?",
                "answer": "Cloud Data Fusion"
            },
            {
                "question": "Which tool to use to provide data lineage ?",
                "answer": "Data Catalog"
            },
            {
                "question": "Which tool to use to protect sensitive data ?",
                "answer": "Cloud Data Loss Prevention"
            },
            {
                "question": "What is a major limitation when executing jobs on a hadoop cluster ?",
                "answer": "There is no separation between compute and storage (it is hard to scale fast)"
            },
            {
                "question": "When using Dataproc, why shouldn't you use hadoop commands directly but instead prefer Google Cloud commands ?",
                "answer": "To allow Dataproc to collect metadata for cluster management purposes"
            },
            {
                "question": "Jobs are not restartable by default, if you enable job restart what should you do when designing them ?",
                "answer": "You should make sure they are idempotent"
            },
            {
                "question": "What are some steps you can take to optimize a dataproc job ?",
                "answer": "Make sure it is close to cloud storage, that there are no funnels, les than 10K input files, benchmark the disk size and computing power. You can also create a cluster for a job"
            },
            {
                "question": "What are the few reasons to use HDFS with dataproc ?",
                "answer": "Frequent directory renaming, data appending to files, heavy IO, low latency needs."
            },
            {
                "question": "When working with dataproc, what is an IDLE cluster ?",
                "answer": "A cluster not working"
            },
            {
                "question": "Can dataproc auto-scaling scale to zero ?",
                "answer": "No, you may want to terminate a temporary cluster instead of scaling it to zero"
            },
            {
                "question": "Which open-source project is DataFlow based on ?",
                "answer": "Apache Beam"
            },
            {
                "question": "In DataFlow, what is a P Collection ?",
                "answer": "An immutable dataset"
            },
            {
                "question": "In DataFlow, what is a P Transform ?",
                "answer": "An action (code) performed on a P Collection"
            },
            {
                "question": "In DataFlow, what is a bounded collection ?",
                "answer": "A batch dataset"
            },
            {
                "question": "In DataFlow, what is an unbounded collection ?",
                "answer": "A streaming dataset"
            },
            {
                "question": "In DataFlow, why is data stored as a serialized byte string ?",
                "answer": "To avoid serialization/deserialization operations"
            },
            {
                "question": "When programming a DataFlow pipeline with Python, how do you chain P transforms ?",
                "answer": "By using the pipe '|' operator"
            },
            {
                "question": "In DataFlow, what is a side input ?",
                "answer": "An additional inputs that fits in memory (can be an aggregation provided by a preceeding P Transform)"
            },
            {
                "question": "In DataFlow, why should you use Combine instead of GroupBy when performing a reduction ?",
                "answer": "GroupBy will shuffle elements and one worker may have many more elements to compute than another. Combine requires an associative and commutative function but avoid the shuffling."
            },
            {
                "question": "Which software is Cloud Composer based on ?",
                "answer": "Apache Airflow"
            },
            {
                "question": "Which tool to use to build pipelines in no-code ?",
                "answer": "Cloud Data Fusion"
            },
            {
                "question": "What are the different ways to schedule jobs in Airflow ?",
                "answer": "On a periodic basis or with triggers"
            },
            {
                "question": "What are 5 criteria to consider when it comes to data quality ?",
                "answer": "Validity, Accuracy, Completeness, Consistency, Uniformity"
            }
        ]
    }
]